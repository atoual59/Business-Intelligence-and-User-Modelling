{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommenders 4 : Pytorch and Recommenders (~1h)\n",
    "\n",
    "In this practical session, we dive a little more into [pytorch](https://pytorch.org/docs/stable/index.html) and propose to re-implement two classical matrix-factorization models with this neural network toolkit.\n",
    "\n",
    "## Objectives:\n",
    "\n",
    "- (a) See a bit of simple pytorch (~5min)\n",
    "- (b) Discover the \"autograd\" part of pytorch to build a simple baseline (~20min)\n",
    "- (c) Discover the \"nn\" part of pytorch to build a simple matrix factorization algorithm (~20min)\n",
    "- (d) Learn to use a high level framework for pytorch (kind of \"KERAS\" like) to build more complicated algorithms (~15min)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-1.8.0-cp38-none-macosx_10_9_x86_64.whl (119.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 119.6 MB 46.8 MB/s eta 0:00:01  |█▏                              | 4.5 MB 5.2 MB/s eta 0:00:23     |████████████████▉               | 62.9 MB 22.0 MB/s eta 0:00:03\n",
      "\u001b[?25hCollecting torchvision\n",
      "  Downloading torchvision-0.9.0-cp38-cp38-macosx_10_9_x86_64.whl (13.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 13.2 MB 17.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pytorch-lightning\n",
      "  Downloading pytorch_lightning-1.2.4-py3-none-any.whl (829 kB)\n",
      "\u001b[K     |████████████████████████████████| 829 kB 21.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting future>=0.17.1\n",
      "  Downloading future-0.18.2.tar.gz (829 kB)\n",
      "\u001b[K     |████████████████████████████████| 829 kB 18.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting PyYAML!=5.4.*,>=5.1\n",
      "  Downloading PyYAML-5.3.1.tar.gz (269 kB)\n",
      "\u001b[K     |████████████████████████████████| 269 kB 42.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorboard>=2.2.0\n",
      "  Downloading tensorboard-2.4.1-py3-none-any.whl (10.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 10.6 MB 34.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting fsspec[http]>=0.8.1\n",
      "  Downloading fsspec-0.8.7-py3-none-any.whl (103 kB)\n",
      "\u001b[K     |████████████████████████████████| 103 kB 28.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tqdm>=4.41.0\n",
      "  Downloading tqdm-4.59.0-py2.py3-none-any.whl (74 kB)\n",
      "\u001b[K     |████████████████████████████████| 74 kB 6.7 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.16.6 in /Users/mac/miniconda3/envs/bima/lib/python3.8/site-packages (from pytorch-lightning) (1.20.1)\n",
      "Collecting typing-extensions\n",
      "  Using cached typing_extensions-3.7.4.3-py3-none-any.whl (22 kB)\n",
      "Collecting aiohttp\n",
      "  Downloading aiohttp-3.7.4.post0-cp38-cp38-macosx_10_14_x86_64.whl (648 kB)\n",
      "\u001b[K     |████████████████████████████████| 648 kB 12.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests in /Users/mac/miniconda3/envs/bima/lib/python3.8/site-packages (from fsspec[http]>=0.8.1->pytorch-lightning) (2.25.1)\n",
      "Requirement already satisfied: six>=1.10.0 in /Users/mac/miniconda3/envs/bima/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.15.0)\n",
      "Collecting grpcio>=1.24.3\n",
      "  Downloading grpcio-1.36.1-cp38-cp38-macosx_10_10_x86_64.whl (3.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.8 MB 15.6 MB/s eta 0:00:01     |██████████████████              | 2.1 MB 15.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting werkzeug>=0.11.15\n",
      "  Downloading Werkzeug-1.0.1-py2.py3-none-any.whl (298 kB)\n",
      "\u001b[K     |████████████████████████████████| 298 kB 18.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-auth<2,>=1.6.3\n",
      "  Downloading google_auth-1.28.0-py2.py3-none-any.whl (136 kB)\n",
      "\u001b[K     |████████████████████████████████| 136 kB 35.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting absl-py>=0.4\n",
      "  Downloading absl_py-0.12.0-py3-none-any.whl (129 kB)\n",
      "\u001b[K     |████████████████████████████████| 129 kB 10.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: setuptools>=41.0.0 in /Users/mac/miniconda3/envs/bima/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning) (50.3.0.post20201006)\n",
      "Requirement already satisfied: wheel>=0.26 in /Users/mac/miniconda3/envs/bima/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.35.1)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.3-py2.py3-none-any.whl (18 kB)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.0-py3-none-any.whl (781 kB)\n",
      "\u001b[K     |████████████████████████████████| 781 kB 21.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting protobuf>=3.6.0\n",
      "  Downloading protobuf-3.15.6-cp38-cp38-macosx_10_9_x86_64.whl (1.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.0 MB 18.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.3.4-py3-none-any.whl (97 kB)\n",
      "\u001b[K     |████████████████████████████████| 97 kB 7.1 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.7.2-py3-none-any.whl (34 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "\u001b[K     |████████████████████████████████| 155 kB 17.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting cachetools<5.0,>=2.0.0\n",
      "  Downloading cachetools-4.2.1-py3-none-any.whl (12 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
      "Collecting pyasn1<0.5.0,>=0.4.6\n",
      "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "\u001b[K     |████████████████████████████████| 77 kB 12.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: chardet<5,>=3.0.2 in /Users/mac/miniconda3/envs/bima/lib/python3.8/site-packages (from requests->fsspec[http]>=0.8.1->pytorch-lightning) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/mac/miniconda3/envs/bima/lib/python3.8/site-packages (from requests->fsspec[http]>=0.8.1->pytorch-lightning) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/mac/miniconda3/envs/bima/lib/python3.8/site-packages (from requests->fsspec[http]>=0.8.1->pytorch-lightning) (1.26.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/mac/miniconda3/envs/bima/lib/python3.8/site-packages (from requests->fsspec[http]>=0.8.1->pytorch-lightning) (2020.12.5)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.1.0-py2.py3-none-any.whl (147 kB)\n",
      "\u001b[K     |████████████████████████████████| 147 kB 24.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pillow>=4.1.1 in /Users/mac/miniconda3/envs/bima/lib/python3.8/site-packages (from torchvision) (7.2.0)\n",
      "Collecting yarl<2.0,>=1.0\n",
      "  Downloading yarl-1.6.3-cp38-cp38-macosx_10_14_x86_64.whl (124 kB)\n",
      "\u001b[K     |████████████████████████████████| 124 kB 30.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-5.1.0-cp38-cp38-macosx_10_14_x86_64.whl (49 kB)\n",
      "\u001b[K     |████████████████████████████████| 49 kB 4.0 MB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /Users/mac/miniconda3/envs/bima/lib/python3.8/site-packages (from aiohttp->fsspec[http]>=0.8.1->pytorch-lightning) (20.2.0)\n",
      "Collecting async-timeout<4.0,>=3.0\n",
      "  Downloading async_timeout-3.0.1-py3-none-any.whl (8.2 kB)\n",
      "Building wheels for collected packages: future, PyYAML\n",
      "  Building wheel for future (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491059 sha256=0c5e4e0b7c4d17749b29096fb452756faad6293b012ae3470b65aaea9501bfd5\n",
      "  Stored in directory: /Users/mac/Library/Caches/pip/wheels/8e/70/28/3d6ccd6e315f65f245da085482a2e1c7d14b90b30f239e2cf4\n",
      "  Building wheel for PyYAML (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for PyYAML: filename=PyYAML-5.3.1-cp38-cp38-macosx_10_9_x86_64.whl size=155647 sha256=d8f0f0aa8bb1f760397b04114e8725f74f25a02e8c0511484740bd1b23de8432\n",
      "  Stored in directory: /Users/mac/Library/Caches/pip/wheels/13/90/db/290ab3a34f2ef0b5a0f89235dc2d40fea83e77de84ed2dc05c\n",
      "Successfully built future PyYAML\n",
      "Installing collected packages: pyasn1, rsa, pyasn1-modules, oauthlib, multidict, cachetools, yarl, typing-extensions, requests-oauthlib, google-auth, async-timeout, werkzeug, tensorboard-plugin-wit, protobuf, markdown, grpcio, google-auth-oauthlib, fsspec, aiohttp, absl-py, tqdm, torch, tensorboard, PyYAML, future, torchvision, pytorch-lightning\n",
      "Successfully installed PyYAML-5.3.1 absl-py-0.12.0 aiohttp-3.7.4.post0 async-timeout-3.0.1 cachetools-4.2.1 fsspec-0.8.7 future-0.18.2 google-auth-1.28.0 google-auth-oauthlib-0.4.3 grpcio-1.36.1 markdown-3.3.4 multidict-5.1.0 oauthlib-3.1.0 protobuf-3.15.6 pyasn1-0.4.8 pyasn1-modules-0.2.8 pytorch-lightning-1.2.4 requests-oauthlib-1.3.0 rsa-4.7.2 tensorboard-2.4.1 tensorboard-plugin-wit-1.8.0 torch-1.8.0 torchvision-0.9.0 tqdm-4.59.0 typing-extensions-3.7.4.3 werkzeug-1.0.1 yarl-1.6.3\n",
      "Requirement already satisfied: matplotlib in /Users/mac/miniconda3/envs/bima/lib/python3.8/site-packages (3.3.1)\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.3.4-cp38-cp38-macosx_10_9_x86_64.whl (8.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 8.5 MB 2.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pillow>=6.2.0 in /Users/mac/miniconda3/envs/bima/lib/python3.8/site-packages (from matplotlib) (7.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /Users/mac/miniconda3/envs/bima/lib/python3.8/site-packages (from matplotlib) (2.8.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /Users/mac/miniconda3/envs/bima/lib/python3.8/site-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: numpy>=1.15 in /Users/mac/miniconda3/envs/bima/lib/python3.8/site-packages (from matplotlib) (1.20.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/mac/miniconda3/envs/bima/lib/python3.8/site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/mac/miniconda3/envs/bima/lib/python3.8/site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: six in /Users/mac/miniconda3/envs/bima/lib/python3.8/site-packages (from cycler>=0.10->matplotlib) (1.15.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing collected packages: matplotlib\n",
      "  Attempting uninstall: matplotlib\n",
      "    Found existing installation: matplotlib 3.3.1\n",
      "    Uninstalling matplotlib-3.3.1:\n",
      "      Successfully uninstalled matplotlib-3.3.1\n",
      "Successfully installed matplotlib-3.3.4\n"
     ]
    }
   ],
   "source": [
    "! pip install torch torchvision pytorch-lightning --upgrade\n",
    "! pip install matplotlib --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (a) WHAT IS PYTORCH?\n",
    "\n",
    "It’s a Python-based scientific computing package targeted at two sets of audiences:\n",
    "\n",
    "- A replacement for NumPy to use the power of GPUs\n",
    "- a deep learning research platform that provides maximum flexibility and speed\n",
    "\n",
    "### Tensors : the main unit\n",
    "\n",
    "Tensors are similar to NumPy’s ndarrays, with the addition being that Tensors can also be used on a GPU to accelerate computing.\n",
    "\n",
    "\n",
    "## Some useful functions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### initialize an empty 4x2 matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.1019e-44, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00]])\n"
     ]
    }
   ],
   "source": [
    "x_empty = torch.empty(4, 2)\n",
    "print(x_empty)  #Tensor is not initialized => contains gibberish"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create a 3x2 tensor filled with zeros of type long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0],\n",
      "        [0, 0],\n",
      "        [0, 0]])\n"
     ]
    }
   ],
   "source": [
    "x0 = torch.zeros(3, 2, dtype=torch.long)\n",
    "print(x0) #Tensor has only zeros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create a tensor of size 2 with (0 => 5.5) and (1 => 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5.5000, 3.0000])\n"
     ]
    }
   ],
   "source": [
    "x_data = torch.tensor([5.5, 3])\n",
    "print(x_data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5.5000, 3.0000], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "x_data = torch.tensor(np.array([5.5, 3])) #also works with numpy arrays\n",
    "print(x_data) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create random 5x3 and 3x5 tensors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5283, 0.8717, 0.6568],\n",
      "        [0.9527, 0.9021, 0.0667],\n",
      "        [0.2179, 0.0246, 0.4431],\n",
      "        [0.1012, 0.9802, 0.1448],\n",
      "        [0.6364, 0.3324, 0.3857]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(5,3)\n",
    "y = torch.rand(3,5)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Indexing works just like numpy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.8717, 0.9021, 0.0246, 0.9802, 0.3324])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:,1] #The 2nd column (indexing starts at 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9527, 0.9021, 0.0667],\n",
       "        [0.1012, 0.9802, 0.1448]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[[1,3],:] # the 2nd and 4th row "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "scalar = torch.tensor([1])\n",
    "print(scalar.item()) # Gets the value when tensor is a scalar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### know the size of a tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.size() ## equivalent to x.shape in numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### simple addition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.5283, 1.8717, 1.6568],\n",
       "        [1.9527, 1.9021, 1.0667],\n",
       "        [1.2179, 1.0246, 1.4431],\n",
       "        [1.1012, 1.9802, 1.1448],\n",
       "        [1.6364, 1.3324, 1.3857]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x+1        # same as x.add(1)\n",
    "x.add_(1)  # inplace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### matrix multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.9293, 2.9642, 1.5680, 0.4155, 2.2434],\n",
       "        [1.6849, 2.7233, 1.5736, 0.2850, 1.9592],\n",
       "        [1.5314, 2.0201, 0.9843, 0.3584, 1.8633],\n",
       "        [1.4751, 2.6686, 1.5037, 0.2897, 1.6121],\n",
       "        [1.6863, 2.3437, 1.2246, 0.3526, 2.0382]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mm(x,y) # same as x @ y or np.dot(x.numpy(),y.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What to understand:\n",
    "\n",
    "Pytorch can be a drop-in replacement for numpy. It behaves mostly the same and the API is close.\n",
    "\n",
    "\n",
    "### There are many more creation/operation ops:\n",
    "\n",
    "=> You can have a look at the [torch.Tensor documentation](https://pytorch.org/docs/stable/tensors.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's interesting beyond the \"numpy replacement\": autodiff !\n",
    "\n",
    "Pytorch has Automatic differentiation: You only have to compute a loss function to obtain gradients automatically. How it works is detailed [here](https://pytorch.org/tutorials/beginner/pytorch_with_examples.html#pytorch-tensors-and-autograd)\n",
    "\n",
    "### Let's do 1d-linear regression with the vanilla autodiff !\n",
    "\n",
    "#### (First) we need fake data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x124eb4a90>]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fake_x = np.arange(0,100,1)\n",
    "fake_y = np.arange(0,10,0.1) + np.random.rand(100)\n",
    "plt.plot(fake_x,fake_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** Here, we won't split the data in train/val/test, this is just an example\n",
    "\n",
    "So, the linear model we want to learn is the following:\n",
    "$$f(x) =  wx+b $$\n",
    "The parameters to optimize are w and b."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.], requires_grad=True) tensor([0.5000], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# First we create to tensor variables (which are our parameters)\n",
    "w = torch.tensor([1.],requires_grad=True) # We need to set requires_grad to True so the gradient can flow.\n",
    "b = torch.tensor([0.5],requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define the f function:\n",
    "def f(x):\n",
    "    return (w*x)+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define an error function (here the MAE)\n",
    "def error(pred,real):\n",
    "    return (pred-real).abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      "loss: 0.3386442530155182\n",
      "w: 0.10660025477409363\n",
      "b: 0.4888014793395996\n",
      "----\n",
      "loss: 0.38207367300987244\n",
      "w: 0.09800027310848236\n",
      "b: 0.4888014793395996\n",
      "----\n",
      "loss: 0.3538042640686035\n",
      "w: 0.10220028460025787\n",
      "b: 0.4894014000892639\n",
      "----\n",
      "loss: 0.35267513692379\n",
      "w: 0.10220029950141907\n",
      "b: 0.48960137367248535\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1255daca0>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Finally, we cycle through the data, optimizing the parameters with respect to the gradient of the error:\n",
    "plt.plot(fake_y)\n",
    "\n",
    "for epoch in range(4): # We cycle 4 times\n",
    "    mean_loss = 0\n",
    "    for x,y in zip(fake_x,fake_y): \n",
    "        pred = f(x) #predict\n",
    "        loss = error(pred,y) #compute loss\n",
    "        loss.backward() # This does backpropagation and sets .grad attribute.\n",
    "\n",
    "        # Update parameters via SGD:\n",
    "        with torch.no_grad(): # This deactivated gradient calculations\n",
    "            \n",
    "            mean_loss += loss.item() # get the raw value of a (1,) tensor\n",
    "            w -= 0.0001 * w.grad # This wouldn't be possible w/ gradient (-= is an inplace operation)\n",
    "            b -= 0.0001 * b.grad\n",
    "            w.grad.zero_()\n",
    "            b.grad.zero_()\n",
    "            \n",
    "    # Plot the resulting line     \n",
    "    predictions = [f(x).detach().numpy() for x in range(100)]\n",
    "    plt.plot(predictions,label=f\"epoch {epoch}\")\n",
    "    print('----')\n",
    "    print(\"loss:\", mean_loss/len(fake_y))\n",
    "    print('w:',w.item())\n",
    "    print('b:',b.item())\n",
    "    \n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Full pytorch tutorial: \n",
    "\n",
    "A tutorial can be found [here](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html) do not hesitate to take a couple of minutes to skim read it. Plenty of [ressources](https://pytorch.org/resources) are available online. Also, you can have a look at the [extensive pytorch documentation](https://pytorch.org/docs/stable/index.html). \n",
    "\n",
    "Here, as we are defining neural networks, we mainly use the `torch.nn` module which contains most classical deep learning building blocks\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data used : [smallest movie-lens dataset](https://grouplens.org/datasets/movielens/)\n",
    "\n",
    "=> Just like the previous sessions\n",
    "\n",
    "\n",
    "# 1)  Load & Prepare Data\n",
    "\n",
    "To be able to embed the data easily, we need to remap  the user/items between [0->N_User] and [0->N_Items]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-156-fb69dc967d20>, line 9)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-156-fb69dc967d20>\"\u001b[0;36m, line \u001b[0;32m9\u001b[0m\n\u001b[0;31m    item_map{:2}\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from random import shuffle\n",
    "\n",
    "## Load\n",
    "ratings = pd.read_csv(\"dataset/ratings.csv\")\n",
    "print(ratings.head(5))\n",
    "## Prepare Data\n",
    "user_map = {user:num for num,user in enumerate(ratings[\"userId\"].unique())}\n",
    "item_map = {item:num for num,item in enumerate(ratings[\"movieId\"].unique())}\n",
    "\n",
    "## Number of users & items\n",
    "num_users = len(user_map)\n",
    "num_items = len(item_map)\n",
    "\n",
    "ratings[\"userId\"] = ratings[\"userId\"].map(user_map)\n",
    "ratings[\"movieId\"] = ratings[\"movieId\"].map(item_map)\n",
    "\n",
    "\n",
    "# Creating Test/Train as before\n",
    "\n",
    "train_indexes,val_indexes,test_indexes = [],[],[]\n",
    "\n",
    "for index in range(len(ratings)):\n",
    "    if index%5 == 0:\n",
    "        test_indexes.append(index)\n",
    "    else:\n",
    "        train_indexes.append(index)\n",
    "\n",
    "        \n",
    "shuffle(train_indexes)\n",
    "num_val = int(len(train_indexes)/100*20)\n",
    "val_indexes = train_indexes[:num_val]\n",
    "train_indexes = train_indexes[num_val:]\n",
    "\n",
    "train_ratings = ratings.iloc[train_indexes].copy()\n",
    "val_ratings = ratings.iloc[val_indexes].copy()\n",
    "test_ratings = ratings.iloc[test_indexes].copy()\n",
    "\n",
    "print(f\" #train:{len(train_ratings)}, #val:{len(val_ratings)} ,#test:{len(test_ratings)}\" )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (TODO) Reproduce the baseline model with pytorch's vanilla autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your goal now is to reproduce the following baseline model from surprise\n",
    "\n",
    "## $$\\hat{r}_{ui} = b_{ui} = \\mu + b_u + b_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (TODO) : First, let's define the parameters\n",
    "\n",
    "You have many parameters, they are all 1-dimensional:\n",
    "- **mu:** the global mean (1,)\n",
    "- **bu:** the user means (n_users,)\n",
    "- **bi:** the item means (n_items,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.5001, dtype=torch.float64, requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "mu = torch.tensor(train_ratings['rating'].mean(),requires_grad=True)\n",
    "print(mu)\n",
    "bu = [ torch.tensor(train_ratings.loc[train_ratings['userId'] == u]['rating'].mean(),requires_grad=True) for u in range(num_users)]\n",
    "bi = []\n",
    "for i in range(num_items):\n",
    "    moy = train_ratings.loc[train_ratings['movieId'] == L[i]]['rating'].mean()\n",
    "    if(not math.isnan(moy)):\n",
    "        bi.append(torch.tensor(moy,requires_grad=True))\n",
    "    else:\n",
    "        bi.append(torch.tensor(0.,requires_grad=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Then, we define two functions: \n",
    "\n",
    "- `predict(u,i)` : Will return the prediction given the (user,item) pair\n",
    "- `error(pred,real)` : Will return the MSE error of prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (TODO) Predict Function\n",
    "This function should implement this: $\\hat{r}_{ui} = b_{ui} = \\mu + b_u + b_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(u,i):\n",
    "    \n",
    "    if u < num_users: # if user exist:\n",
    "        user_mean = bu[u]\n",
    "    else:\n",
    "        user_mean = 0\n",
    "        \n",
    "    if i < num_items: # if item exist:\n",
    "        item_mean = bi[i]\n",
    "    else:\n",
    "        item_mean = 0\n",
    "        \n",
    "    return mu.item()+user_mean+item_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (TODO) error function\n",
    "We want to use the MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error(pred,real):\n",
    "    return (pred-real)**2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The evaluation loop, without any optimization for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final train error :  38.76785971837357\n",
      "final val error :  38.96689720459835\n",
      "final test error :  38.737803739685354\n"
     ]
    }
   ],
   "source": [
    "train_e = 0\n",
    "for index, uid, mid, r, ts in train_ratings.itertuples():\n",
    "    result = predict(uid,mid)\n",
    "    train_e += error(result,r).item()\n",
    "    \n",
    "val_e = 0\n",
    "for index, uid, mid, r, ts in val_ratings.itertuples():\n",
    "    result = predict(uid,mid)\n",
    "    val_e += error(result,r).item()\n",
    "\n",
    "test_e = 0\n",
    "for index, uid, mid, r, ts in test_ratings.itertuples():\n",
    "    result = predict(uid,mid)\n",
    "    test_e += error(result,r).item()\n",
    "\n",
    "print(\"final train error : \", train_e/len(train_ratings))\n",
    "print(\"final val error : \", val_e/len(val_ratings))\n",
    "print(\"final test error : \", test_e/len(test_ratings))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's optimize the parameters (with SGD)  by slightly modifying the previous loop\n",
    "\n",
    "### (TODO)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 train error :  38.76785971837357\n",
      "epoch 0 val error :  34.54314793229203\n",
      "epoch 0 test error :  34.323287068776445\n",
      "-----\n",
      "epoch 1 train error :  38.76785971837357\n",
      "epoch 1 val error :  30.65322955083963\n",
      "epoch 1 test error :  30.451557919609588\n",
      "-----\n",
      "epoch 2 train error :  38.76785971837357\n",
      "epoch 2 val error :  27.642443777926072\n",
      "epoch 2 test error :  27.440557042533978\n",
      "-----\n",
      "epoch 3 train error :  38.76785971837357\n",
      "epoch 3 val error :  25.140626557769068\n",
      "epoch 3 test error :  24.94180524865986\n",
      "-----\n",
      "epoch 4 train error :  38.76785971837357\n",
      "epoch 4 val error :  23.10409574296881\n",
      "epoch 4 test error :  22.90651464725815\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "lr = 0.01\n",
    "batch_size = 32\n",
    "n_epochs = 5\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    train_error = 0\n",
    "    \n",
    "    for num,(index, uid, mid, r, ts) in enumerate(train_ratings.sample(frac=1).itertuples()):\n",
    "        result = predict(uid,mid)\n",
    "        ex_error =error(result,r)\n",
    "        train_error +=  ex_error\n",
    "        \n",
    "        ex_error.backward()\n",
    "\n",
    "        if num % batch_size == 0:\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                bu[uid] -= 0.0001 * bu[uid].grad\n",
    "                bi[mid] -= 0.0001 * bi[mid].grad\n",
    "\n",
    "                # Manually zero the gradients after updating weights\n",
    "                bu[uid].grad.zero_()\n",
    "                bi[mid].grad.zero_()\n",
    "\n",
    "\n",
    "    print(f\"epoch {epoch} train error : \", train_e/len(train_ratings))\n",
    "    \n",
    "    val_e = 0\n",
    "    for index, uid, mid, r, ts in val_ratings.itertuples():\n",
    "        result = predict(uid,mid)\n",
    "        val_e += error(result,r).item()\n",
    "\n",
    "    print(f\"epoch {epoch} val error : \", val_e/len(val_ratings))\n",
    "\n",
    "\n",
    "    test_e = 0\n",
    "    for index, uid, mid, r, ts in test_ratings.itertuples():\n",
    "        result = predict(uid,mid)\n",
    "        test_e += error(result,r).item()\n",
    "\n",
    "    print(f\"epoch {epoch} test error : \", test_e/len(test_ratings))\n",
    "    print(\"-----\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Pytorch (.nn) Modules\n",
    "\n",
    "Instead of having to define everything by hand, pytorch has several usefull abstractions:\n",
    "\n",
    "- `nn.Module()` -> To define the model and the forward computation\n",
    "- `torch.utils.data.DataLoader` -> To create the data pipeline\n",
    "\n",
    "To explore these modules, we'll do the following model:\n",
    "\n",
    "##  Classic SVD (with mean)\n",
    "\n",
    "To see how it works, we propose to implement a simple SVD:\n",
    "### $$ \\min\\limits_{U,I}\\sum\\limits_{(u,i)} \\underbrace{(r_{ui} -  (I_i^TU_u + \\mu))^2}_\\text{minimization} + \\underbrace{\\lambda(||U_u||^2+||I_u||^2 + \\mu) }_\\text{regularization} $$\n",
    "\n",
    "where prediction is done in the following way:\n",
    "### $$r_{ui} = \\mu + U_u.I_i $$\n",
    "\n",
    "where $\\mu$ is the global mean,  $U_u$ a user embedding and $I_i$ an item embedding\n",
    "\n",
    "### STEPS:\n",
    " To implement such model in pytorch, we need to do multiple things:\n",
    " \n",
    " - (1) model definition\n",
    " - (2) loss function\n",
    " - (3) evaluation\n",
    " - (4) training/eval loop\n",
    "\n",
    "\n",
    "#### (1) Model definition\n",
    "\n",
    "A model class typically extends `nn.Module`, the Neural network module. It is a convenient way of encapsulating parameters, with helpers for moving them to GPU, exporting, loading, etc.\n",
    "\n",
    "One should define two functions: `__init__` and `forward`.\n",
    "\n",
    "- `__init__` is used to initialize the model parameters\n",
    "- `forward` is the net transformation from input to output. In fact, when doing `moduleClass(input)` you call this method.\n",
    "\n",
    "##### (a) Initialization\n",
    "\n",
    "Our model has different weigths:\n",
    "\n",
    "- the user profiles (also called user embeddings) $U$\n",
    "- the item profiles (also called user embeddings) $I$\n",
    "- the mean bias $\\mu$\n",
    "\n",
    "\n",
    "##### (b) input to output operation\n",
    "Technically, the prediction as defined earlier can be seen as just a dot product between two embeddings $U_u$ and $I_i$ plus the mean rating:\n",
    "\n",
    "- `torch.sum(embed_u*embed_i,1) + self.mean` is equivalent to $r_{ui} = \\mu + U_u.I_i $ \n",
    "- the `.squeeze(1)` operation is a shape operation to remove the dimension 1 (indexing starts at 0) akin to reshaping the matrix from `(batch_size,1,latent_size)` to `(batch_size,latent_size)`\n",
    "- for reference, the inverse operation is `.unsqueeze()`\n",
    "- we return weights to regularize them\n",
    "\n",
    "\n",
    "### (TODO) Just to make sure you were following: complete the following `forward` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Let's create the datasets following  (Object w/ __getitem__(index) and __len()__, i.e lists ;)\n",
    "prep_train = [(tp.userId,tp.movieId,tp.rating) for tp in train_ratings.itertuples()]\n",
    "prep_val = [(tp.userId,tp.movieId,tp.rating) for tp in val_ratings.itertuples()]\n",
    "prep_test = [(tp.userId,tp.movieId,tp.rating) for tp in test_ratings.itertuples()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "# The model define as a class, inheriting from nn.Module\n",
    "class ClassicMF(nn.Module):\n",
    "    \n",
    "    #(a) Init\n",
    "    def __init__(self,nb_users,nb_items,latent_size):\n",
    "        super(ClassicMF, self).__init__()\n",
    "        \n",
    "        #Embedding layers\n",
    "        self.users = nn.Embedding(#To Complete, latent_size)        \n",
    "        self.items = nn.Embedding(#To Complete, latent_size)\n",
    "        #The mean bias\n",
    "        self.mean = nn.Parameter(torch.FloatTensor(1,).fill_(3))\n",
    "        \n",
    "        #initialize weights with very small values\n",
    "        nn.init.normal_(self.users.weight,0,0.01)\n",
    "        nn.init.normal_(self.items.weight,0,0.01)\n",
    "\n",
    "    \n",
    "    # (b) How we compute the prediction (from input to output)\n",
    "    def forward(self, user, item): ## method called when doing ClassicMF(user,item)\n",
    "        \n",
    "        embed_u,embed_i = self.users(user).squeeze(1),self.items(item).squeeze(1)\n",
    "        out =  #To Complete\n",
    "\n",
    "        return out, embed_u, embed_i, self.mean  # We return prediction + weights to regularize them\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2-4) full train loop\n",
    "\n",
    "The train loop is organized around the [Dataloader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) class which Combines a dataset and a sampler, and provides single- or multi-process iterators over the dataset.\n",
    "\n",
    "We just redefine a collate function\n",
    "\n",
    "> collate_fn (callable, optional) – merges a list of samples to form a mini-batch.\n",
    "\n",
    "\n",
    "**NOTE:** The dataset argument can be a list instead of a \"Dataset\" instance (works by duck typing)\n",
    "    \n",
    "\n",
    "##### The train loop sequence is the following:\n",
    "    \n",
    "[Dataset ==Dataloader==> Batch (not prepared) ==collate_fn==> Batch (prepared) ==Model.forward==> Prediction =loss_fn=> loss <-> truth \n",
    "\n",
    "1] PREDICT\n",
    "- (a) The dataloader samples training exemples from the dataset (which is a list)\n",
    "- (b) The collate_fn prepares the minibatch of training exemples\n",
    "- (c) The prediction is made by feeding the minibatch in the model\n",
    "- (d) The loss is computed on the prediction via a loss function\n",
    "\n",
    "2] OPTIMIZE\n",
    "- (e) Gradients are computed by automatic backard propagation\n",
    "- (f) Parameters are updated using computed gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# HyperParameters\n",
    "n_epochs = 3\n",
    "batch_size = 16\n",
    "num_feat = 25\n",
    "lr = 0.01\n",
    "reg = 0.001\n",
    "\n",
    "\n",
    "#(b) Collate function => Creates tensor batches to feed model during training\n",
    "# It can be removed if data is already tensors (torch or numpy ;)\n",
    "def tuple_batch(l):\n",
    "    '''\n",
    "    input l: list of (user,item,rating tuples)\n",
    "    output: formatted batches (in torch tensors)\n",
    "\n",
    "    takes n-tuples and create batch\n",
    "    text -> seq word #id\n",
    "    '''\n",
    "    users, items, ratings = zip(*l) \n",
    "    users_t = torch.LongTensor(users)\n",
    "    items_t = torch.LongTensor(items)\n",
    "    ratings_t = torch.FloatTensor(ratings)\n",
    "    \n",
    "    return users_t, items_t, ratings_t\n",
    "    \n",
    "\n",
    "\n",
    "#(d) Loss function => Combines MSE and L2\n",
    "def loss_func(pred,ratings_t,reg,*params):\n",
    "    '''\n",
    "    mse loss combined with l2 regularization.\n",
    "    params assumed 2-dimension\n",
    "    '''\n",
    "    mse = F.mse_loss(pred,ratings_t,reduction='sum')\n",
    "    l2 = 0\n",
    "    for p in params:\n",
    "        l2 += torch.mean(p.norm(2,-1))\n",
    "        \n",
    "    return (mse/pred.size(0)) + reg*l2 , mse\n",
    "    \n",
    "#\n",
    "# Training script starts here\n",
    "#    \n",
    "\n",
    "\n",
    "model = ClassicMF(num_users,num_items,num_feat)\n",
    "\n",
    "\n",
    "\n",
    "# (a) dataloader will sample data from datasets using collate_fn tuple_batch\n",
    "dataloader_train = DataLoader(prep_train, batch_size=batch_size, shuffle=True, num_workers=0, collate_fn=tuple_batch)\n",
    "dataloader_val = DataLoader(prep_val, batch_size=batch_size, shuffle=True, num_workers=0, collate_fn=tuple_batch)\n",
    "dataloader_test = DataLoader(prep_test, batch_size=batch_size, shuffle=False, num_workers=0, collate_fn=tuple_batch)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "# Train loop\n",
    "for e in range(n_epochs):\n",
    "    mean_loss = [0,0,0] #train/val/test\n",
    "\n",
    "    ## Training loss (the one we train with)\n",
    "    \n",
    "    for users_t,items_t,ratings_t in dataloader_train:\n",
    "        model.train() # set the model on train mode\n",
    "        model.zero_grad() # reset gradients\n",
    "        \n",
    "        #(c) predictions are made by the model\n",
    "        pred,*params = model(users_t,items_t)\n",
    "        \n",
    "        #(d) loss computed on predictions, we added regularization\n",
    "        loss,mse_loss = loss_func(pred,ratings_t,reg,*params)\n",
    "        \n",
    "        loss.backward() #(e) backpropagating to get gradients\n",
    "        \n",
    "        mean_loss[0] += mse_loss\n",
    "        optimizer.step() #(f) updating parameters\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        ## Validation loss (no training)\n",
    "        for users_t,items_t,ratings_t in dataloader_val:\n",
    "\n",
    "            model.eval() # Inference mode\n",
    "            pred,*params = model(users_t,items_t)\n",
    "            _,mse_loss = loss_func(pred,ratings_t,reg,*params)\n",
    "\n",
    "            mean_loss[1] += mse_loss    \n",
    "\n",
    "        ## Test loss (no training)\n",
    "\n",
    "        for users_t,items_t,ratings_t in dataloader_test:\n",
    "            model.eval()\n",
    "            pred,*params = model(users_t,items_t)\n",
    "            _,mse_loss = loss_func(pred,ratings_t,reg,*params)\n",
    "\n",
    "            mean_loss[2] += mse_loss    \n",
    "\n",
    "    print(\"-\"*25)\n",
    "    print(\"epoch\",e, \"mse (train/val/test)\", round((mean_loss[0]/len(prep_train)).item(),3),\"/\",  round((mean_loss[1]/len(prep_val)).item(),3),\"/\",  round((mean_loss[2]/len(prep_test)).item(),3))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Your turn) Koren 2009 model:\n",
    "\n",
    "Here, this model simply adds a bias for each user and for each item\n",
    "\n",
    "### $$ \\min\\limits_{U,I}\\sum\\limits_{(u,i)} \\underbrace{(r_{ui} -  (I_i^TU_u + \\mu+ \\mu_i+\\mu_u))^2}_\\text{minimization} + \\underbrace{\\lambda(||U_u||^2+||I_u||^2 + \\mu  + \\mu+ \\mu_i+\\mu_u) }_\\text{regularization} $$\n",
    "\n",
    "\n",
    "### $$r_{ui} = \\mu + \\mu_i + \\mu_u + U_u.I_i $$\n",
    "\n",
    "### TODO:\n",
    "\n",
    "- (a) complete the model initialization\n",
    "- (b) complete the forward method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KorenMF(nn.Module):\n",
    "\n",
    "    def __init__(self,nb_users,nb_items,latent_size):\n",
    "        super(KorenMF, self).__init__()\n",
    "        \n",
    "        self.users = #To Complete\n",
    "        self.items = #To Complete\n",
    "        self.umean = #To Complete\n",
    "        self.imean = #To Complete\n",
    "        self.gmean =  #To Complete\n",
    "\n",
    "        nn.init.normal_(self.users.weight,0,0.01)\n",
    "        nn.init.normal_(self.items.weight,0,0.01)\n",
    "        nn.init.normal_(self.umean.weight,2,1)\n",
    "        nn.init.normal_(self.imean.weight,2,1)\n",
    "        \n",
    "        \n",
    "    def forward(self, user,item):\n",
    "        embed_u,embed_i = self.users(user).squeeze(1) , self.items(item).squeeze(1)\n",
    "        umean, imean = self.umean(user) , self.imean(item)\n",
    "        out = #To Complete\n",
    "\n",
    "        return out , embed_u, embed_i, umean , imean , self.gmean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (TODO) Here, train loop stays the same, you only have to change the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "n_epochs = 10\n",
    "batch_size = 16\n",
    "num_feat = 25\n",
    "lr = 0.01\n",
    "reg = 0.001\n",
    "\n",
    "\n",
    "\n",
    "def tuple_batch(l):\n",
    "    '''\n",
    "    input l: list of (user,item,review, rating tuples)\n",
    "    output: formatted batches (in torch tensors)\n",
    "\n",
    "    takes n-tuples and create batch\n",
    "    text -> seq word #id\n",
    "    '''\n",
    "    users, items,ratings = zip(*l)\n",
    "    users_t = torch.LongTensor(users)\n",
    "    items_t = torch.LongTensor(items)\n",
    "    ratings_t = torch.FloatTensor(ratings)\n",
    "    \n",
    "    return users_t,items_t,ratings_t\n",
    "\n",
    "\n",
    "def loss_func(pred,ratings_t,reg,*params):\n",
    "    '''\n",
    "    mse loss combined with l2 regularization.\n",
    "    params assumed 2-dimension\n",
    "    '''\n",
    "    mse = F.mse_loss(pred,ratings_t,reduction=\"sum\")\n",
    "    l2 = 0\n",
    "    for p in params:\n",
    "        l2 += torch.mean(p.norm(2,-1))\n",
    "        \n",
    "    return (mse/pred.size(0)) + reg*l2 , mse\n",
    "    \n",
    "\n",
    "model =  #To Complete\n",
    "\n",
    "\n",
    "dataloader_train = DataLoader(prep_train, batch_size=batch_size, shuffle=True, num_workers=0, collate_fn=tuple_batch)\n",
    "dataloader_val = DataLoader(prep_val, batch_size=batch_size, shuffle=False, num_workers=0, collate_fn=tuple_batch)\n",
    "dataloader_test = DataLoader(prep_test, batch_size=batch_size, shuffle=False, num_workers=0, collate_fn=tuple_batch)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "\n",
    "for e in range(n_epochs):\n",
    "    mean_loss = [0,0,0] #train/val/test\n",
    "\n",
    "    for users_t,items_t,ratings_t in dataloader_train:\n",
    "        model.train()\n",
    "        model.zero_grad()\n",
    "        pred,*params = model(users_t,items_t)\n",
    "\n",
    "        loss,mse_loss = loss_func(pred,ratings_t,reg,*params)\n",
    "        loss.backward()\n",
    "        \n",
    "        mean_loss[0] += mse_loss\n",
    "        optimizer.step()\n",
    "    \n",
    "    \n",
    "\n",
    "    for users_t,items_t,ratings_t in dataloader_val:\n",
    "        model.eval()\n",
    "        pred,*params = model(users_t,items_t)\n",
    "        _,mse_loss = loss_func(pred,ratings_t,reg,*params)\n",
    "    \n",
    "        mean_loss[1] += mse_loss    \n",
    "        \n",
    "    for users_t,items_t,ratings_t in dataloader_test:\n",
    "        model.eval()\n",
    "        pred,*params = model(users_t,items_t)\n",
    "        _,mse_loss = loss_func(pred,ratings_t,reg,*params)\n",
    "    \n",
    "        mean_loss[2] += mse_loss    \n",
    "\n",
    "    print(\"-\"*25)\n",
    "    print(\"epoch\",e, \"mse (train/val/test)\", round((mean_loss[0]/len(prep_train)).item(),3),\"/\",  round((mean_loss[1]/len(prep_val)).item(),3),\"/\",  round((mean_loss[2]/len(prep_test)).item(),3))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch's keras: Pytorch-Lightning\n",
    "\n",
    "Pytorch lightning is a easy to use framework for Pytorch. To start a new project you just need to define two files:\n",
    "\n",
    "- a LightningModule (which inherits `pl.LightningModule`)\n",
    "- a Trainer file. \n",
    "\n",
    "By defining those two files, you get:\n",
    "- Checkpointing\n",
    "- Debugging\n",
    "- Distributed training\n",
    "- Experiment Logging\n",
    "- Training loop\n",
    "- Validation loop\n",
    "- Testing loop\n",
    "\n",
    "## Let's try with the same but different Koren 2009 model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### $$r_{ui} = \\mu + \\mu_i + \\mu_u + U_u.I_i $$\n",
    "\n",
    "Where the goal is to minimize the following loss\n",
    "\n",
    "### $$ \\min\\limits_{U,I}\\sum\\limits_{(u,i)} \\underbrace{(r_{ui} -  (I_i^TU_u + \\mu+ \\mu_i+\\mu_u))^2}_\\text{minimization} + \\underbrace{\\lambda(||U_u||^2+||I_u||^2 + \\mu  + \\mu+ \\mu_i+\\mu_u) }_\\text{regularization} $$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (TODO) Complete the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "class LightningKorenMF(pl.LightningModule):\n",
    "\n",
    "    def __init__(self,nb_users,nb_items,latent_size):\n",
    "        super(LightningKorenMF, self).__init__()\n",
    "        \n",
    "        self.reg = 0.001\n",
    "        \n",
    "        self.users = #To Complete\n",
    "        self.items = #To Complete\n",
    "        self.umean = #To Complete\n",
    "        self.imean = #To Complete\n",
    "        self.gmean =  nn.Parameter(torch.FloatTensor(1,).fill_(3))\n",
    "\n",
    "        nn.init.normal_(self.users.weight,0,0.01)\n",
    "        nn.init.normal_(self.items.weight,0,0.01)\n",
    "        nn.init.normal_(self.umean.weight,0.1,0.1)\n",
    "        nn.init.normal_(self.imean.weight,0.1,0.1)\n",
    "        \n",
    "\n",
    "    def forward(self, user,item):\n",
    "        embed_u,embed_i = self.users(user).squeeze(1) , self.items(item).squeeze(1)\n",
    "        umean, imean = self.umean(user) , self.imean(item)\n",
    "        out = #To Complete\n",
    "\n",
    "        return out , embed_u, embed_i, umean , imean , self.gmean\n",
    "\n",
    "    \n",
    "    def my_loss_func(self, pred,ratings_t,reg,*params):\n",
    "        '''\n",
    "        mse loss combined with l2 regularization.\n",
    "        params assumed 2-dimension\n",
    "        '''        \n",
    "        mse = F.mse_loss(pred,ratings_t)\n",
    "        l2 = 0\n",
    "        for p in params:\n",
    "            l2 += torch.mean(p.norm(2,-1))\n",
    "\n",
    "        return mse + reg*l2 , mse\n",
    "    \n",
    "    def training_step(self, batch, batch_nb):\n",
    "        # REQUIRE\n",
    "        users_t,items_t,ratings_t = batch\n",
    "        pred , *params = self.forward(users_t,items_t) \n",
    "        loss,mse = self.my_loss_func(pred,ratings_t,self.reg,*params)\n",
    "\n",
    "        return {'loss':loss,\"mse\":mse}\n",
    "    \n",
    "\n",
    "    \n",
    "    def validation_step(self, batch, batch_nb):\n",
    "        return {\"val_mse\":self.training_step(batch,batch_nb)[\"mse\"]}\n",
    "    \n",
    "    def validation_end(self,outputs):\n",
    "        return {\"progress_bar\":{\"val_mse\":torch.tensor([output['val_mse'] for output in outputs]).mean().item()}}\n",
    "    \n",
    "    def test_step(self, batch, batch_nb):\n",
    "        return {\"test_mse\":self.training_step(batch,batch_nb)[\"mse\"]}\n",
    "    \n",
    "    def test_end(self,outputs):\n",
    "        res = {\"progress_bar\":{\"test_mse\":torch.tensor([output['test_mse'] for output in outputs]).mean().item()}}\n",
    "        print(res)\n",
    "        return res\n",
    "\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        # REQUIRED\n",
    "        return torch.optim.Adam(self.parameters(), lr=0.002)\n",
    "    \n",
    "    def tuple_batch(self,l):\n",
    "        '''\n",
    "        input l: list of (user,item,rating tuples)\n",
    "        output: formatted batches (in torch tensors)\n",
    "\n",
    "        takes n-tuples and create batch\n",
    "        text -> seq word #id\n",
    "        '''\n",
    "        users, items, ratings = zip(*l) \n",
    "        users_t = torch.LongTensor(users)\n",
    "        items_t = torch.LongTensor(items)\n",
    "        ratings_t = torch.FloatTensor(ratings)\n",
    "\n",
    "        return users_t, items_t, ratings_t\n",
    "    \n",
    "\n",
    "    @pl.data_loader\n",
    "    def train_dataloader(self):\n",
    "        # REQUIRED\n",
    "        return DataLoader(prep_train,collate_fn=self.tuple_batch ,num_workers=0, batch_size=32)\n",
    "\n",
    "    @pl.data_loader\n",
    "    def val_dataloader(self):\n",
    "        # OPTIONAL\n",
    "        return DataLoader(prep_val,collate_fn=self.tuple_batch,num_workers=0, batch_size=32)\n",
    "\n",
    "    @pl.data_loader\n",
    "    def test_dataloader(self):\n",
    "        # OPTIONAL\n",
    "        return DataLoader(prep_test,collate_fn=self.tuple_batch,num_workers=0, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning import Trainer\n",
    "\n",
    "model = LightningKorenMF(num_users,num_items,50)\n",
    "\n",
    "# most basic trainer, uses good defaults\n",
    "trainer = Trainer()    \n",
    "trainer.fit(model)\n",
    "trainer.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Still got time ?\n",
    "\n",
    "[Take a glance at the documentation](https://williamfalcon.github.io/pytorch-lightning/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
